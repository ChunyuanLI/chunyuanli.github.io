<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Chunyuan  Li


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li> -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Chunyuan</span>  Li
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/chunyl-1400x1153.jpg" srcset="    /assets/resized/chunyl-480x395.jpg 480w,    /assets/resized/chunyl-800x659.jpg 800w,    /assets/resized/chunyl-1400x1153.jpg 1400w,/ assets/img/chunyl.jpg 2995w">

      
      
    </div>
    

    <div class="clearfix">
      <p>I am a principal researcher at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/" target="_blank" rel="noopener noreferrer">Microsoft Research, Redmond</a>. My recent research focuses on large-scale pre-training in computer vision and natural language processing. Some recent works include:</p>

<ul>
  <li>Vision-and-language pre-training [<a href="https://arxiv.org/abs/2111.11432" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://www.microsoft.com/en-us/research/blog/objects-are-the-secret-key-to-revealing-the-world-between-vision-and-language/" target="_blank" rel="noopener noreferrer">2</a>, <a href="https://arxiv.org/abs/2112.03857" target="_blank" rel="noopener noreferrer">3</a>]</li>

  <li>Deep generative models at scale [<a href="http://chunyuan.li/doc/Scaling_DGMs_chunyl.pdf" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://www.microsoft.com/en-us/research/blog/a-deep-generative-model-trifecta-three-advances-that-work-towards-harnessing-large-scale-power/" target="_blank" rel="noopener noreferrer">2</a>, 
    <a href="https://arxiv.org/abs/2111.13792" target="_blank" rel="noopener noreferrer">3</a>
    <a href="https://gligen.github.io/" target="_blank" rel="noopener noreferrer">4</a>
  ]</li>
    <li>Self-supervised visual representation learning [<a href="https://arxiv.org/abs/2106.09785" target="_blank" rel="noopener noreferrer">1</a>]</li>
</ul>

<p>I obtained my PhD in machine learning at <a href="https://duke.edu/" target="_blank" rel="noopener noreferrer">Duke University</a>, advised by Prof. <a href="http://people.ee.duke.edu/~lcarin" target="_blank" rel="noopener noreferrer">Lawrence Carin</a>. My PhD research studies <a href="http://chunyuan.li/doc/PhD_research_cli.pdf" target="_blank" rel="noopener noreferrer">probabilistic deep learning</a>. 
I have served as an Area Chair for NeurIPS, ICML, EMNLP &amp; AAAI, and a Guest Editor of <a href="https://www.springer.com/journal/11263/updates/23503548" target="_blank" rel="noopener noreferrer">IJCV</a>.</p>



<hr>

<img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20">
              <em>International Journal of Computer Vision (IJCV)</em> special issue on 
 <a href="https://www.springer.com/journal/11263/updates/23503548" target="_blank" rel="noopener noreferrer">``Promises and Dangers of Large Vision Modelsâ€™â€™</a>. Submission deadline: March 1st, 2023 <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20"> 


      
<hr>


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
        
        
      
        <tr>
          <th scope="row">Jan 17, 2023</th>
          <td>

          Papers are released:
          <ul>
            <li>
              <a href="https://react-vl.github.io/" target="_blank" rel="noopener noreferrer">REACT</a> improves foundation models on various vision tasks by customizing them with retrieval-augmented multimodal knowledge
            </li>
            <li>
              <a href="https://gligen.github.io/" target="_blank" rel="noopener noreferrer">GLIGEN</a> enables a new capability for frozen text-to-image generation models: open-set grounding. <a href="https://aka.ms/gligen" target="_blank" rel="noopener noreferrer">[Demo]</a>
            </li>
            <li>
              <a href="https://x-decoder-vl.github.io/" target="_blank" rel="noopener noreferrer">X-Decoder</a>: a generalist decoder for pixel, image and language <a href="https://huggingface.co/spaces/xdecoder/Demo" target="_blank" rel="noopener noreferrer">[Demo]</a>
            </li>            
          </ul>


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 23, 2022</th>
          <td>
            
             <p>ECCV Workshop and Challenge on <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild (CVinW)</a>. For those who are new to this topic, please check out the
          <a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/blob/main/README.md" target="_blank">
      CVinW Reading List
          </a>. </p>

        <table>
          <tr>
            <td style="width:220px">  <center>
        <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer"><img src="https://computer-vision-in-the-wild.github.io/eccv-2022/static/eccv2022/img/ECCV-logo3.png" alt="CVinW" width="100" height="52">   [Workshop]</a>  </center> </td>
            <td style="width:220px"> <center>
        <a href="https://eval.ai/web/challenges/challenge-page/1832/overview" target="_blank" rel="noopener noreferrer"><img src="https://evalai.s3.amazonaws.com/media/logos/4e939412-a9c0-46bd-9797-5ba0bd0a9095.jpg" alt="ICinW" width="100" height="52"> [IC Challenge]</a>   </center> </td>
            <td style="width:220px">  <center>
        <a href="https://eval.ai/web/challenges/challenge-page/1839/overview" target="_blank" rel="noopener noreferrer"><img src="https://evalai.s3.amazonaws.com/media/logos/e3727105-2b29-4c9b-98a6-3d1191884eb5.jpg" alt="ODinW" width="100" height="52"> [OD Challenge]</a>  </center> </td>
          </tr>
        </table>


            
          </td>
        </tr>

        <tr>
          <th scope="row">Oct 17, 2022</th>
          <td>
            
             <a href="https://arxiv.org/abs/2210.09263" target="_blank" rel="noopener noreferrer">"Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends"</a>,  A 100-page survey paper in <a href="https://www.nowpublishers.com/article/Details/CGV-105" target="_blank" rel="noopener noreferrer"><em>Foundations and TrendsÂ® in Computer Graphics and Vision</em></a>
            
          </td>
        </tr>

        <tr>
          <th scope="row">Sep 16, 2022</th>
          <td>
            
              NeurIPS 2022: <a href="https://arxiv.org/abs/2204.09222" target="_blank" rel="noopener noreferrer">K-LITE (Oral, 1%)</a>,  <a href="https://arxiv.org/abs/2204.08790" target="_blank" rel="noopener noreferrer">ELEVATER</a>  and <a href="arXiv%20preprint%20arXiv:2203.11926">FocalNet</a>. A team effort to push <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">CVinW</a>. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">; <a href="https://datarelease.blob.core.windows.net/tutorial/VLP-Tutorial_2022/vlp_for_v_part3.pdf" target="_blank" rel="noopener noreferrer">[CVPR Tutorial]</a>
              <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">


<ul>
  <li>
    <a href="https://arxiv.org/abs/2204.09222" target="_blank" rel="noopener noreferrer">K-LITE</a> demonstrates the effectiveness of external knowledge to improve language-image models in zero-/few-shot task transfer
  </li>
  <li>
    <a href="https://arxiv.org/abs/2204.08790" target="_blank" rel="noopener noreferrer">ELEVATER</a> is a platform with 20 image classification and 35 object detection public datasets for evaluating language-image models in task-level visual transfer. <a href="https://computer-vision-in-the-wild.github.io/ELEVATER/" target="_blank" rel="noopener noreferrer">[Benchmark Website]</a>
  </li>
  <li>
  FocalNet [<a href="https://arxiv.org/abs/2203.11926">paper</a>, <a href="https://github.com/microsoft/FocalNet" target="_blank" rel="noopener noreferrer">code</a>, <a href="https://huggingface.co/spaces/jw2yang/focalnet-modulators" target="_blank" rel="noopener noreferrer">demo</a>, <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/focalnets-focusing-the-eyes-with-focal-modulation/" target="_blank" rel="noopener noreferrer">blog</a>]  - SoTA on COCO object detection with a simple attention-free architecture
  </li>
</ul>

            
          </td>
        </tr>
      
      
        <tr>
          <th scope="row">Mar 25, 2022</th>
          <td>
            
              Upcoming events as a co-organizer:
<ul>
  <li>CVPR 2022 tutorial on <a href="https://vlp-tutorial.github.io/" target="_blank" rel="noopener noreferrer">Recent Advances in Vision-and-Language Pre-training</a> 
</li>
  <li>ECCV 2022 workshop on <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild</a>
</li>
  <li>ICDM 2022 workshop on <a href="https://fomo-vl.github.io/icdm2022/" target="_blank" rel="noopener noreferrer">Foundation Models in Vision and Language</a>
</li>
</ul>
          </td>
        </tr>
      
 
      
        <tr>
          <th scope="row">Mar 1, 2022</th>
          <td>
            
              CVPR 2022:
<ul>
  <li>
    UniCL [<a href="https://arxiv.org/abs/2204.03610" target="_blank" rel="noopener noreferrer">paper</a>, <a href="github.com/microsoft/UniCL">code</a>, <a href="https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog" target="_blank" rel="noopener noreferrer">demo</a>]
introduces the unified language-image-label contrast; UniCL is an academic-friendly version of <a href="https://arxiv.org/abs/2111.11432" target="_blank" rel="noopener noreferrer">[Microsoft Florence]</a>:
  </li>
  <li>
    GLIP [<a href="https://arxiv.org/abs/2112.03857">paper</a>, <a href="https://github.com/microsoft/GLIP" target="_blank" rel="noopener noreferrer">code</a>, <a href="hhttps://huggingface.co/spaces/haotiz/glip-zeroshot-demo" target="_blank" rel="noopener noreferrer">demo</a>] Oral Presentation, CVPR Best Paper Finalist
  </li>
  <li>
    RegionCLIP [<a href="https://arxiv.org/abs/2112.09106" target="_blank" rel="noopener noreferrer">paper</a>, <a href="https://github.com/microsoft/RegionCLIP" target="_blank" rel="noopener noreferrer">code</a>, <a href="https://huggingface.co/spaces/CVPR/regionclip-demo" target="_blank" rel="noopener noreferrer">demo</a>]
  </li>
  <li>
    Lafite [<a href="https://arxiv.org/abs/2111.13792" target="_blank" rel="noopener noreferrer">paper</a>, <a href="https://github.com/drboog/Lafite" target="_blank" rel="noopener noreferrer">code</a>]
  </li>
</ul>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">June 17, 2021</th>
          <td>
            
             <a href="https://arxiv.org/abs/2210.09263" target="_blank" rel="noopener noreferrer">EsViT</a> chieves SoTA 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with an order magnitude of higher throughput.  [<a href="https://github.com/microsoft/esvit" target="_blank" rel="noopener noreferrer">GitHub</a>]
            
          </td>
        </tr>

      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>recent publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">K-LITE</abbr>
    
  
  </div>

  <div id="shen2022klite" class="col-sm-8">
    
      <div class="title">K-LITE: Learning Transferable Visual Models with External Knowledge</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Shen, Sheng*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Chunyuan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hu, Xiaowei*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Yujia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Jianwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Pengchuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rohrbach, Anna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gan, Zhe,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Lijuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuan, Lu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Ce,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Keutzer, Kurt,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Darrell, Trevor,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gao, Jianfeng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>NeurIPS</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2204.09222" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ELEVATER</abbr>
    
  
  </div>

  <div id="li2022elevater" class="col-sm-8">
    
      <div class="title">ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Chunyuan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Haotian*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Liunian Harold,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Pengchuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Aneja, Jyoti,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Jianwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jin, Ping,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Yong Jae,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hu, Houdong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zicheng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gao, Jianfeng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>NeurIPS (Datasets and Benchmarks Track)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2204.08790" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">UniCL</abbr>
    
  
  </div>

  <div id="yang2022unified" class="col-sm-8">
    
      <div class="title">Unified Contrastive Learning in Image-Text-Label Space</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yang, Jianwei*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Chunyuan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Pengchuan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiao, Bin*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Ce,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuan, Lu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gao, Jianfeng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>CVPR</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://arxiv.org/abs/2204.03610" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/microsoft/UniCL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6C%69%63%68%75%6E%79%75%61%6E%32%34@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/ChunyuanLI" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/chunyuan-li-039b6a44" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/ChunyuanLi" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>













      </div>
      <div class="contact-note">Email is the best way to reach me.
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2022 Chunyuan  Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
