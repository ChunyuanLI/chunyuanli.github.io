<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Chunyuan  Li


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->
<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li> -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
  
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Chunyuan</span>  Li
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/chunyl-1400x1153.jpg" srcset="    /assets/resized/chunyl-480x395.jpg 480w,    /assets/resized/chunyl-800x659.jpg 800w,    /assets/resized/chunyl-1400x1153.jpg 1400w,/ assets/img/chunyl.jpg 2995w">

      
      
    </div>
    

    <div class="clearfix">
      <p>My research centers on multimodal intelligence, with a focus on large-scale language and vision training. Key contributions include <a href="https://llava-vl.github.io/" target="_blank" rel="noopener noreferrer">LLaVA</a> and <a href="https://llava-vl.github.io/blog/" target="_blank" rel="noopener noreferrer">its model family series</a>, as well as foundational early work such as <a href="https://github.com/IDEA-Research/GroundingDINO" target="_blank" rel="noopener noreferrer">GroundingDINO</a>, <a href="https://github.com/microsoft/GLIP" target="_blank" rel="noopener noreferrer">GLIP</a>, <a href="https://gligen.github.io/" target="_blank" rel="noopener noreferrer">GLIGEN</a>, <a href="https://arxiv.org/abs/2111.11432">Florence</a>, and <a href="https://www.microsoft.com/en-us/research/blog/objects-are-the-secret-key-to-revealing-the-world-between-vision-and-language" target="_blank" rel="noopener noreferrer">Oscar</a>.
      </p>
<p>My experience includes research roles at <a href="https://xai.com/" target="_blank" rel="noopener noreferrer">xAI</a>, <a href="https://seed.bytedance.com/en/public_papers/seed1-5-vl-technical-report" target="_blank" rel="noopener noreferrer">ByteDance</a>, and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/" target="_blank" rel="noopener noreferrer">Microsoft Research, Redmond</a>. I earned my PhD in machine learning from <a href="https://duke.edu/" target="_blank" rel="noopener noreferrer">Duke University</a> under the guidance of Prof. <a href="http://people.ee.duke.edu/~lcarin" target="_blank" rel="noopener noreferrer">Lawrence Carin</a>, where my doctoral research explored <a href="http://chunyuan.li/doc/PhD_research_cli.pdf" target="_blank" rel="noopener noreferrer">deep generative models</a>. 
I have also served the community as an Area Chair for NeurIPS, ICML, ICLR, EMNLP, TMLR and a Guest Editor of <a href="https://www.springer.com/journal/11263/updates/23503548" target="_blank" rel="noopener noreferrer">IJCV</a> on ``the promises and dangers of large vision models''.</p>




<!-- <hr>

<img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20">
              <em>International Journal of Computer Vision (IJCV)</em> special issue on 
 <a href="https://www.springer.com/journal/11263/updates/23503548" target="_blank" rel="noopener noreferrer">``Promises and Dangers of Large Vision Modelsâ€™â€™</a>. Submission deadline: April 1st, 2023 <img class="emoji" title=":pencil:" alt=":pencil:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png" height="20" width="20">  -->


      
<hr>


    </div>


  
    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        <tr>
          <th scope="row">2025</th>
          <td>
            <a href="https://x.ai/news/grok-3" target="_blank" rel="noopener noreferrer">Grok-3</a>: Visual understanding and Realtime video in voice-mode.
          </td>
          
        </tr>

        <tr>
          <th scope="row">2024</th>
          <td>
            Exploring the boundaries of fully open-source VLMs to establish a mature recipe, documented in <a href="https://llava-vl.github.io/blog/" target="_blank" rel="noopener noreferrer">Blog Series</a> and <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" target="_blank" rel="noopener noreferrer">Github</a>
            <ul>
              <li> LLaVA-NeXT, LLaVA-OneVision, LLaVA-Video, LLaVA-Critic </li>
            </ul>
            Developing the proprietary industry-leading VLM in image and video understanding:  <a href="https://arxiv.org/abs/2505.07062" target="_blank" rel="noopener noreferrer">Seed-VL-1.5</a>
          </td>
          
        </tr>

        <tr>
          <th scope="row">Oct/Nov, 2023</th>
          <td>

          LLaVA is upgraded:
          <ul>
            <li>
              <a href="https://llava-vl.github.io/" target="_blank" rel="noopener noreferrer">LLaVA-1.5</a> achieves SoTA on 11 benchmarks among open-source VLMs. It utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses prior SoTA that use billion-scale data.
            	[<a href="https://llava-vl.github.io/" target="_blank" rel="noopener noreferrer">Project</a>] 
            	[<a href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener noreferrer">Paper</a>] 
            	[<a href="https://github.com/haotian-liu/LLaVA" target="_blank" rel="noopener noreferrer">Github</a>] 
            	[<a href="https://llava.hliu.cc/" target="_blank" rel="noopener noreferrer">Demo</a>] 
            	[<a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md" target="_blank" rel="noopener noreferrer">Model Zoo</a>] 
            </li>
            <li>
            	<a href="https://llava-vl.github.io/llava-interactive/" target="_blank" rel="noopener noreferrer">LLaVA-Interactive</a>: Experience the future of human-AI multimodal interaction with an all-in-one demo for image chat, segmentation, generation and editing.
            	[<a href="https://llava-vl.github.io/llava-interactive/" target="_blank" rel="noopener noreferrer">Project</a>] 
            	[<a href="https://arxiv.org/abs/2311.00571" target="_blank" rel="noopener noreferrer">Paper</a>] 
            	[<a href="https://github.com/LLaVA-VL/LLaVA-Interactive-Demo" target="_blank" rel="noopener noreferrer">Github</a>] 
            	[<a href="https://llavainteractive.ngrok.io/" target="_blank" rel="noopener noreferrer">Demo</a>] 
            </li>             
            <li>
            	<a href="https://llava-vl.github.io/llava-plus/" target="_blank" rel="noopener noreferrer">LLaVA-Plus</a> expands the capabilities of LLaVA by learning to use external tools for creating multimodal agents. 
            	[<a href="https://llava-vl.github.io/llava-plus/" target="_blank" rel="noopener noreferrer">Project</a>] 
            	[<a href="https://arxiv.org/abs/2311.05437" target="_blank" rel="noopener noreferrer">Paper</a>] 
            	[<a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase" target="_blank" rel="noopener noreferrer">Github</a>] 
            	[<a href="https://llavaplus.ngrok.io/" target="_blank" rel="noopener noreferrer">Demo</a>] 
            </li>
          </ul>


            
          </td>
        </tr>

        <tr>
          <th scope="row">September 20, 2023</th>
          <td>
            A 110-page paper is released to share our perspective on multimodal models: <a href="https://arxiv.org/abs/2309.10020" target="_blank" rel="noopener noreferrer">``Multimodal Foundation Models: From Specialists to General-Purpose Assistants''</a>. This is based our <a href="https://vlp-tutorial.github.io/2023" target="_blank" rel="noopener noreferrer">CVPR 2023 Tutorial</a>.
             [<a href="https://arxiv.org/abs/2306.14895" target="_blank" rel="noopener noreferrer">Note on Large Multimodal Models</a>] [<a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_VLM.pdf" target="_blank" rel="noopener noreferrer">Slides</a>] [<a href="https://youtu.be/mkI7EPD1vp8" target="_blank" rel="noopener noreferrer">YouTube</a>] [<a href="https://www.bilibili.com/video/BV1Ng4y1T7v3/" target="_blank" rel="noopener noreferrer">Bilibili</a>] 
          </td>
        </tr>


          <tr>
            <th scope="row">June 1, 2023</th>          
          <td>
            <a href="https://aka.ms/llava-med" target="_blank" rel="noopener noreferrer">LLaVA-Med</a>: Training a large language-and-vision assistant for biomedicine in one day. NeurIPS 2023 Datasets and Benchmarks Track (Spotlight)
          </td>
        </tr>

        <tr>
          <th scope="row">April 17, 2023</th>
          <td>
            
             <a href="https://llava-vl.github.io/" target="_blank" rel="noopener noreferrer">Visual Instruction Tuning with GPT-4</a>! We release LLaVA, a Large Language-and-Vision Assistant towards multimodal GPT-4 level capabilities. NeurIPS 2023 (Oral Presentation)
             [<a href="https://llava-vl.github.io/" target="_blank" rel="noopener noreferrer">Project</a>] [<a href="https://arxiv.org/abs/2304.08485" target="_blank" rel="noopener noreferrer">Paper</a>] [<a href="https://github.com/haotian-liu/LLaVA" target="_blank" rel="noopener noreferrer">Github</a>] [<a href="https://llava.hliu.cc/" target="_blank" rel="noopener noreferrer">Demo</a>] [<a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" target="_blank" rel="noopener noreferrer">Data</a>] [<a href="https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0" target="_blank" rel="noopener noreferrer">Model</a>] [<a href="https://arxiv.org/abs/2309.09958" target="_blank" rel="noopener noreferrer">Scaling Note</a>]
            
          </td>
        </tr>
              
        
         <tr>
          <th scope="row">April 7, 2023</th>
          <td>
            
             <a href="https://instruction-tuning-with-gpt-4.github.io/" target="_blank" rel="noopener noreferrer">Instruction Tuning with GPT-4</a>!  a "first attempt" to use GPT-4 data for LLM self-instruct tuning. [<a href="https://arxiv.org/abs/2304.03277" target="_blank" rel="noopener noreferrer">Paper</a>] [<a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM" target="_blank" rel="noopener noreferrer">Github</a>] [<a href="https://www.linkedin.com/feed/update/urn:li:activity:7049992414995902464/" target="_blank" rel="noopener noreferrer">My Learnings</a>]
            
          </td>
        </tr>
       
        
      
        <tr>
          <th scope="row">March, 2023</th>
          <td>

          CVPR 2023:
          <ul>
            <li>
              <a href="https://react-vl.github.io/" target="_blank" rel="noopener noreferrer">REACT</a> improves foundation models on various vision tasks by customizing them with retrieval-augmented multimodal knowledge <a href="https://github.com/microsoft/react" target="_blank" rel="noopener noreferrer">[Code]</a> (Highlights, 2.5%)
            </li>
            <li>
              <a href="https://gligen.github.io/" target="_blank" rel="noopener noreferrer">GLIGEN</a> enables a new capability for frozen text-to-image generation models: open-set grounding. <a href="https://aka.ms/gligen" target="_blank" rel="noopener noreferrer">[Demo]</a> <a href="https://github.com/gligen/GLIGEN" target="_blank" rel="noopener noreferrer">[Code]</a> <a href="https://youtu.be/-MCkU7IAGKs" target="_blank" rel="noopener noreferrer">[YouTube]</a>
            </li>
            <li>
              <a href="https://x-decoder-vl.github.io/" target="_blank" rel="noopener noreferrer">X-Decoder</a>: a generalist decoder for pixel, image and language <a href="https://huggingface.co/spaces/xdecoder/Demo" target="_blank" rel="noopener noreferrer">[Demo]</a> <a href="https://github.com/microsoft/X-Decoder" target="_blank" rel="noopener noreferrer">[Code]</a>
            </li>            
          </ul>


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb, 2023</th>
          <td>
            
             <p>CVPR2023 Workshop and Challenge on the 2nd <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild (CVinW)</a>. For those who are new to this topic, please check out the
          <a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/blob/main/README.md" target="_blank">
      CVinW Reading List
          </a>.  
            <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/" target="_blank" rel="noopener noreferrer">[Workshop]</a>  
            <a href="https://eval.ai/web/challenges/challenge-page/1931/overview" target="_blank" rel="noopener noreferrer">[SGinW Challenge]</a> 
            <a href="https://eval.ai/web/challenges/challenge-page/1973/overview" target="_blank" rel="noopener noreferrer">[RF100 Challenge]</a> 
        </p>



            
          </td>
        </tr>        
        
        <tr>
          <th scope="row">Oct 23, 2022</th>
          <td>
            
             <p>ECCV 2022 Workshop and Challenge on the 1st <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild (CVinW)</a>. Please check out the videos of this event at
          <a href="https://www.youtube.com/@cvinw" target="_blank">[YouTube]</a> <a href="https://space.bilibili.com/628847266/channel/seriesdetail?sid=2887565" target="_blank">[BiliBili]</a>. 
            
          <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">[Workshop]</a>
      <a href="https://eval.ai/web/challenges/challenge-page/1832/overview" target="_blank" rel="noopener noreferrer">[ICinW Challenge]</a>  
      <a href="https://eval.ai/web/challenges/challenge-page/1839/overview" target="_blank" rel="noopener noreferrer">[ODinW Challenge]</a> 
        </p>

          </td>
        </tr>

        <tr>
          <th scope="row">Oct 17, 2022</th>
          <td>
            
             <a href="https://arxiv.org/abs/2210.09263" target="_blank" rel="noopener noreferrer">"Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends"</a>,  A 100-page survey paper in <a href="https://www.nowpublishers.com/article/Details/CGV-105" target="_blank" rel="noopener noreferrer"><em>Foundations and TrendsÂ® in Computer Graphics and Vision</em></a>
            
          </td>
        </tr>

        <tr>
          <th scope="row">Sep 16, 2022</th>
          <td>
            
              NeurIPS 2022: <a href="https://arxiv.org/abs/2204.09222" target="_blank" rel="noopener noreferrer">K-LITE (Oral, 1%)</a>,  <a href="https://arxiv.org/abs/2204.08790" target="_blank" rel="noopener noreferrer">ELEVATER</a>  and <a href="arXiv%20preprint%20arXiv:2203.11926">FocalNet</a>. A team effort to push <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">CVinW</a>. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">; <a href="https://datarelease.blob.core.windows.net/tutorial/VLP-Tutorial_2022/vlp_for_v_part3.pdf" target="_blank" rel="noopener noreferrer">[CVPR Tutorial]</a>
              <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">


<ul>
  <li>
    <a href="https://arxiv.org/abs/2204.09222" target="_blank" rel="noopener noreferrer">K-LITE</a> demonstrates the effectiveness of external knowledge to improve language-image models in zero-/few-shot task transfer
  </li>
  <li>
    <a href="https://arxiv.org/abs/2204.08790" target="_blank" rel="noopener noreferrer">ELEVATER</a> is a platform with 20 image classification and 35 object detection public datasets for evaluating language-image models in task-level visual transfer. <a href="https://computer-vision-in-the-wild.github.io/ELEVATER/" target="_blank" rel="noopener noreferrer">[Benchmark Website]</a>
  </li>
  <li>
  FocalNet [<a href="https://arxiv.org/abs/2203.11926">paper</a>, <a href="https://github.com/microsoft/FocalNet" target="_blank" rel="noopener noreferrer">code</a>, <a href="https://huggingface.co/spaces/jw2yang/focalnet-modulators" target="_blank" rel="noopener noreferrer">demo</a>, <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/focalnets-focusing-the-eyes-with-focal-modulation/" target="_blank" rel="noopener noreferrer">blog</a>]  - SoTA on COCO object detection with a simple attention-free architecture
  </li>
</ul>

            
          </td>
        </tr>
      
      
        <tr>
          <th scope="row">Mar 25, 2022</th>
          <td>
            
              Upcoming events as a co-organizer:
<ul>
  <li>CVPR 2022 tutorial on <a href="https://vlp-tutorial.github.io/" target="_blank" rel="noopener noreferrer">Recent Advances in Vision-and-Language Pre-training</a> 
</li>
  <li>ECCV 2022 workshop on <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/" target="_blank" rel="noopener noreferrer">Computer Vision in the Wild</a>
</li>
  <li>ICDM 2022 workshop on <a href="https://fomo-vl.github.io/icdm2022/" target="_blank" rel="noopener noreferrer">Foundation Models in Vision and Language</a>
</li>
</ul>
          </td>
        </tr>
      
 
      
        <tr>
          <th scope="row">Mar 1, 2022</th>
          <td>
            
              CVPR 2022:
<ul>
  <li>
    UniCL [<a href="https://arxiv.org/abs/2204.03610" target="_blank" rel="noopener noreferrer">paper</a>, <a href="github.com/microsoft/UniCL">code</a>, <a href="https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog" target="_blank" rel="noopener noreferrer">demo</a>]
introduces the unified language-image-label contrast; UniCL is an academic-friendly version of <a href="https://arxiv.org/abs/2111.11432" target="_blank" rel="noopener noreferrer">[Microsoft Florence]</a>:
  </li>
  <li>
    GLIP [<a href="https://arxiv.org/abs/2112.03857">paper</a>, <a href="https://github.com/microsoft/GLIP" target="_blank" rel="noopener noreferrer">code</a>, <a href="hhttps://huggingface.co/spaces/haotiz/glip-zeroshot-demo" target="_blank" rel="noopener noreferrer">demo</a>] Oral Presentation, CVPR Best Paper Finalist
  </li>
  <li>
    RegionCLIP [<a href="https://arxiv.org/abs/2112.09106" target="_blank" rel="noopener noreferrer">paper</a>, <a href="https://github.com/microsoft/RegionCLIP" target="_blank" rel="noopener noreferrer">code</a>, <a href="https://huggingface.co/spaces/CVPR/regionclip-demo" target="_blank" rel="noopener noreferrer">demo</a>]
  </li>
  <li>
    Lafite [<a href="https://arxiv.org/abs/2111.13792" target="_blank" rel="noopener noreferrer">paper</a>, <a href="https://github.com/drboog/Lafite" target="_blank" rel="noopener noreferrer">code</a>]
  </li>
</ul>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">June 17, 2021</th>
          <td>
            
             <a href="https://arxiv.org/abs/2210.09263" target="_blank" rel="noopener noreferrer">EsViT</a> chieves SoTA 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with an order magnitude of higher throughput.  [<a href="https://github.com/microsoft/esvit" target="_blank" rel="noopener noreferrer">GitHub</a>]
            
          </td>
        </tr>

      </table>
    </div>
  
</div>

<div class="social">
  <div class="contact-icons">
    <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
    <a href="https://github.com/ChunyuanLI" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
    <a href="https://www.linkedin.com/in/chunyuan-li-039b6a44" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
    <a href="https://twitter.com/ChunyuanLi" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
    <a href="mailto:%6C%69%63%68%75%6E%79%75%61%6E%32%34@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>
  </div>
</div>  
    

    
  </article>

    </div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2022 Chunyuan  Li.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
